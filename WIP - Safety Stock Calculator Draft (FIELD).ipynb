{"cells":[{"cell_type":"code","source":"import pandas as _hex_pandas\nimport datetime as _hex_datetime\nimport json as _hex_json","execution_count":null,"metadata":{},"outputs":[]},{"cell_type":"code","source":"hex_scheduled = _hex_json.loads(\"false\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_email = _hex_json.loads(\"\\\"example-user@example.com\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_user_attributes = _hex_json.loads(\"{}\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_run_context = _hex_json.loads(\"\\\"logic\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_timezone = _hex_json.loads(\"\\\"Etc/UTC\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_id = _hex_json.loads(\"\\\"0197cc42-6089-7003-bf1a-0d83acfe7b3f\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_project_name = _hex_json.loads(\"\\\"WIP - Safety Stock Calculator Draft (FIELD)\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_status = _hex_json.loads(\"\\\"Exploratory\\\"\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_categories = _hex_json.loads(\"[]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"code","source":"hex_color_palette = _hex_json.loads(\"[\\\"#FC8D62\\\",\\\"#66C2A5\\\",\\\"#8DA0CB\\\",\\\"#E78AC3\\\",\\\"#A6D854\\\",\\\"#FFD92F\\\",\\\"#E5C494\\\",\\\"#B3B3B3\\\"]\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"## **(1) Order History Range**\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\norder_history_start_start = _hex_formatters.str_to_datetime(\"2024-10-01\").date();\norder_history_start_end = _hex_formatters.str_to_datetime(\"2025-05-15\").date();","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(2) Country Input**\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\ncountry_selection = _hex_json.loads(\"[\\\"US\\\"]\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(3) Replenishment Lead Time to Normal (Weeks)**\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\nreplenishment_lead_time_to_normal = _hex_json.loads(\"10\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(4) Lead Time Standard Deviation for Normal (convert percentage to Weeks)**\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\nlead_time_std_for_normal = _hex_json.loads(\"0.2\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(5) Lead Time Standard Deviation for Field Locations (convert percentage to Weeks)**\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\nlead_time_std_for_field = _hex_json.loads(\"0.2\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(6) Service Level, Annual Demand Threshold, SS Coverage Max**\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\nimport json as _hex_json\nservice_level_annual_demand_ss_coverage = _hex_pks.kernel_execution.input_cell.csv_to_dataframe(args=_hex_types.FileToDataFrameArgs.from_dict({**_hex_json.loads(\"{\\\"filename\\\":\\\"01989f47-0351-7bb3-9f04-47e82834890a.csv\\\",\\\"dir\\\":\\\"cell_uploads\\\"}\")}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"service_level_annual_demand_ss_coverage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## (7) Customer Zip Code to Service Center Mapping\n\n","metadata":{}},{"cell_type":"code","source":"import json as _hex_json\n\nimport json as _hex_json\ncustomer_zip_code_sc_mapping = _hex_pks.kernel_execution.input_cell.csv_to_dataframe(args=_hex_types.FileToDataFrameArgs.from_dict({**_hex_json.loads(\"{\\\"filename\\\":\\\"01980aa4-9882-711e-98b3-612ee3756685.csv\\\",\\\"dir\\\":\\\"cell_uploads\\\"}\")}), app_session_token=_hex_APP_SESSION_TOKEN, python_kernel_init_status=_hex_python_kernel_init_status, hex_timezone=_hex_kernel.variable_or_none(\"hex_timezone\", scope_getter=lambda: globals()), interrupt_event=locals().get(\"_hex_interrupt_event\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_zip_code_sc_mapping['customer_zip_code'] = customer_zip_code_sc_mapping['customer_zip_code'].astype(str)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"customer_zip_code_sc_mapping","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **(8) Field Location Transit Times -- Update 'month' as needed**\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     select location_name,\n#     cast(location_id as FLOAT) as location_id,\n#     -- total_lead_time\n#     round((total_lead_time/7),0) as total_lead_time\n#     -- calculate total_lead_time in weeks\n#     from supply_chain.outbound_logistics.fct_obl_contracted_cost\n#     where\n#     lane_type = \"Contracted\"\n#     and month='2025-06-01' --Update as needed\n#     \n#     -- use total_transit_time column? is this in days?\n#     -- same units as the Normal input?\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## [(1) Order History](https://www.instagram.com/accounts/login/?next=https%3A%2F%2Fwww.instagram.com%2Fusc_msba%2F%3Fhl%3Den&is_from_rle)\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     SELECT *\n#     FROM commercial.demand_planning.rep_orders\n#     WHERE 1=1 \n#     AND order_date BETWEEN {{order_history_start_start}} AND {{order_history_start_end}}\n#     and country in ({{country_selection | array}})\n#     AND fulfillment_status not in ('CANCELLED')\n#     AND is_cancelled = 0 -- will be more accurate than fulfillment_status\n#     AND order_type not in ('Reservation')\n#     AND (model_year = 2025 or model_year = 2026)\n#     and fulfillment_status not in ('CUSTOMER_NOT_READY_TO_CONFIRM', 'CUSTOMER_UNRESPONSIVE')\n#     -- and location_code not in ('8101')\n#     \n#     -- do we want to exclude Not Ready to Confirm and Unresponsive Orders -> should we treat these as real orders -> my only gripe is there are some who are just lingering for long periods and might as well be dead orders\n#     -- exclude these orders^^\n#     -- do we only want to include orders that have been delivered? \n#     ORDER BY order_date ASC\n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## [(2) MY25 to MY26 Remapping](https://www.instagram.com/accounts/login/?next=https%3A%2F%2Fwww.instagram.com%2Fusc_msba%2F%3Fhl%3Den&is_from_rle)\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     with mapping_logic_2026 as (\n#     select\n#     config_string as config_string_original,\n#     replace(model_year, '2025', '2026') as model_year,\n#     country,\n#     model,\n#     powertrain,\n#     model_powertrain,\n#     paint_id,\n#     case\n#     when interior_id = 'INT-BMV' then 'INT-BMP'\n#     else interior_id\n#     end as interior_id,\n#     \n#     case\n#     when\n#     powertrain in ('Dual Large', 'Dual Max') and wheels_id = 'WHL-0BS'\n#     then 'WHL-2SS'\n#     else wheels_id\n#     end as wheels_id,\n#     \n#     case\n#     when\n#     powertrain in ('Dual Large', 'Dual Max')\n#     and (audio_id = 'AUD-P01' or roof_id = 'ROOF-DGP') then 'AUD-P01'\n#     else audio_id\n#     end as audio_id,\n#     \n#     case\n#     when\n#     powertrain in ('Dual Large', 'Dual Max')\n#     and (audio_id = 'AUD-P01' or roof_id = 'ROOF-DGP') then 'ROOF-DGP'\n#     else roof_id\n#     end as roof_id,\n#     \n#     case\n#     when\n#     model_powertrain in ('R1T Dual Large', 'R1T Dual Max')\n#     then 'TON-P02'\n#     when tonneau_id = 'TON-P01' then 'TON-P02'\n#     else tonneau_id\n#     end as tonneau_id,\n#     \n#     case\n#     when powertrain in ('Dual Standard') then 'ACTBDG-BRT'\n#     when powertrain in ('Tri Max') then 'ACTBDG-DRK'\n#     else accents_and_badging_id\n#     end as accents_and_badging_id,\n#     \n#     case\n#     -- retain UTL-NULL for base configs\n#     when config_string_original in (\n#     '2025_US_R1T Dual Standard_EXP-LSV_INT-BMV_WHL-0BS_AUD-STD_ROOF-FGP_TON-NULL_ACTBDG-BRT_UTL-NULL', --noqa: LT05\n#     '2025_US_R1T Dual Standard_EXP-LSV_INT-BMP_WHL-0BS_AUD-STD_ROOF-FGP_TON-NULL_ACTBDG-BRT_UTL-NULL', --noqa: LT05\n#     '2025_US_R1S Dual Standard_EXP-LSV_INT-BMV_WHL-0BS_AUD-STD_ROOF-FGP_TON-NULL_ACTBDG-BRT_UTL-NULL', --noqa: LT05\n#     '2025_US_R1S Dual Standard_EXP-LSV_INT-BMP_WHL-0BS_AUD-STD_ROOF-FGP_TON-NULL_ACTBDG-BRT_UTL-NULL' --noqa: LT05\n#     ) then 'UTL-NULL'\n#     -- as long as not base configs..\n#     when model_powertrain = 'R1T Dual Standard' then 'UTL-T01'\n#     when model_powertrain = 'R1S Dual Standard' then 'UTL-S01'\n#     when\n#     model_powertrain in ('R1S Dual Large', 'R1S Dual Max')\n#     then 'UTL-S01'\n#     when\n#     model_powertrain in ('R1T Dual Large', 'R1T Dual Max')\n#     then 'UTL-T01'\n#     else utility_panel_id\n#     end as utility_panel_id\n#     from commercial.demand_planning.rep_orders\n#     where 1=1\n#     and model_year = 2025\n#     and vehicle_generation = 'GEN-2'\n#     -- and order_date between '2024-10-01' and '2025-05-15'\n#     -- and order_date >= '2024-10-01'\n#     and order_type = 'Order'\n#     and config_string is not null\n#     \n#     ),\n#     \n#     mapping_2025_to_2026 as (\n#     select\n#     config_string_original,\n#     concat(\n#     model_year, '_',\n#     country, '_',\n#     model_powertrain, '_',\n#     paint_id, '_',\n#     interior_id, '_',\n#     wheels_id, '_',\n#     audio_id, '_',\n#     roof_id, '_',\n#     tonneau_id, '_',\n#     accents_and_badging_id, '_',\n#     utility_panel_id\n#     ) as mapped_my26_config_string, \n#     model_year, \n#     country,\n#     model_powertrain, \n#     paint_id, \n#     interior_id,\n#     wheels_id, \n#     audio_id,\n#     roof_id, \n#     tonneau_id, \n#     accents_and_badging_id,\n#     utility_panel_id\n#     \n#     from mapping_logic_2026\n#     )\n#     \n#     \n#     -- if I do not select DISTINCT config_string_original it shows me repeats\n#     -- one config_string_original can have multiple rows for some reason\n#     select DISTINCT config_string_original, \n#     mapped_my26_config_string,\n#     model_year, \n#     country,\n#     model_powertrain, \n#     paint_id, \n#     interior_id,\n#     wheels_id, \n#     audio_id,\n#     roof_id, \n#     tonneau_id, \n#     accents_and_badging_id,\n#     utility_panel_id\n#     -- sku_classification\n#     from mapping_2025_to_2026\n#     order by config_string_original asc\n#     \n#     \n#     \n#     \n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **(3) Item Master**\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     SELECT *,\n#     CASE WHEN is_inventory_policy_eligible = 1 THEN 'Inventory Policy Compliant' ELSE 'Not Compliant' END AS inventory_policy_compliance\n#     FROM commercial.demand_planning.rep_item_master\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **(3) Product Plan**\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     select *, case when is_product_plan_eligible = 1 then 'Product Plan Compliant' else 'Not Compliant' end as product_plan_compliance\n#     from commercial.demand_planning.rep_configurations\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Notes: **\n\n(1) For rep_orders, how come I already filtered out 'CANCELLED' orders for fulfilment_status but I see a cancellation_date on some orders?\n\n- use is_cancelled instead should be more accurate as a flag as opposed to fulfillment_status\n\n(2) MY25-MY26 mapping has duplicates\n\n- Sorting the MY25-MY26 mapping in ASC order shows multiples\n- Correct to just use DISTINCT to get a 1:1 mapping view? -> yes\n\n(3) Not sure if every MY25 is mapped to a MY26\n\n- Seeing some null values in mapped_my26_config_string when I perform a join \n- remove order_date filter on MY25-MY26 mapping\n\n(4)  Assumption before was that item_master contains every possible config_string; I wanted to use a join to item_master to easily split the remapped config_string into individual attributes but looks like there are null values showing up\n\n- This should be fixed once item_master upstream is updated to have corrected MY25-MY26 mapping\n\n","metadata":{}},{"cell_type":"markdown","source":"## **merged_df is the joins of all the above tables with order_history**\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     -- Need to be join by location_type (Normal vs Field, model (R1S vs R1T), sku_classification \n#     \n#     with merged_table as (SELECT \n#     mr.mapped_my26_config_string, oh.config_string,\n#     -- concat(COALESCE(mr.mapped_my26_config_string, oh.config_string), ' | ', zc.sc_name) as unique_config_string_location,\n#     oh.config_string as original_config_string,\n#     coalesce(mr.mapped_my26_config_string, oh.config_string) as mapped_my26_config_string,\n#     \n#     \n#     -- oh.zip_code as order_history_zip_code,\n#     -- manual intervention to address how one single zip code causes an entire column to be read as a float\n#     case when oh.zip_code = '33076-4619' then '33076' \n#     WHEN SUBSTRING(oh.zip_code, 1, 1) = '0' THEN SUBSTRING(oh.zip_code, 2, LEN(oh.zip_code))\n#     else oh.zip_code end as order_history_zip_code,\n#     \n#     \n#     -- zc.customer_zip_code as zip_code_to_sc_mapping_code,\n#     -- zc.sap_sc_code,\n#     -- zc.sc_name,\n#     strftime('%Y-%m-%d', date_trunc('week', order_date)) as order_week,\n#     count(distinct order_week) over () as num_of_weeks,\n#     oh.config_string,\n#     mr.mapped_my26_config_string, \n#     -- concat(mr.mapped_my26_config_string, ' | ', oh.location_name) as unique_config_string_location, \n#     -- realized if i don't filter to the distinct unique_config_string_location i'll just have a lot of repeat rows in my merged_df_2 output\n#     oh.vehicle_generation,\n#     COALESCE(mr.model_year, oh.model_year) as model_year,\n#     coalesce(mr.country, oh.country) as country, \n#     coalesce(mr.model_powertrain, oh.model_powertrain) as model_powertrain,\n#     coalesce(mr.paint_id, oh.paint_id) as paint_id,\n#     coalesce(mr.interior_id, oh.interior_id) as interior_id,\n#     coalesce(mr.wheels_id, oh.wheels_id) as wheels_id, \n#     coalesce(mr.audio_id, oh.audio_id) as audio_id,\n#     coalesce(mr.roof_id, oh.roof_id) as roof_id, \n#     coalesce(mr.tonneau_id, oh.tonneau_id) as tonneau_id, \n#     coalesce(mr.accents_and_badging_id, oh.accents_and_badging_id) as accents_and_badging_id,\n#     coalesce(mr.utility_panel_id, oh.utility_panel_id) as utility_panel_id,\n#     oh.location_name,\n#     im.inventory_policy_compliance, \n#     pp.product_plan_compliance,\n#     -- im.sku_classification,\n#     -- ADD CALCULATED SKU CLASS?\n#     CASE WHEN oh.location_name LIKE ('%Normal%') THEN 'Normal' ELSE 'Field' END AS location_type\n#     FROM order_history_df oh\n#     LEFT JOIN my_25_to_my_26_remapping mr ON oh.config_string = mr.config_string_original\n#     LEFT JOIN item_master im ON im.config_string = mr.mapped_my26_config_string\n#     LEFT JOIN product_plan pp ON pp.config_string = mr.mapped_my26_config_string\n#     -- LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(oh.zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     )\n#     \n#     SELECT distinct concat(COALESCE(mt.mapped_my26_config_string, mt.original_config_string), ' | ', zc.sc_name) as unique_config_string_location,\n#     mt.mapped_my26_config_string,\n#     mt.original_config_string,\n#     mt.order_history_zip_code,\n#     zc.customer_zip_code as zip_code_to_sc_mapping_code,\n#     zc.sap_sc_code,\n#     zc.sc_name,\n#     mt.num_of_weeks,\n#     mt.vehicle_generation,\n#     mt.model_year,\n#     mt.country,\n#     substring(mt.model_powertrain, 1, 3) as model,\n#     mt.model_powertrain,\t\n#     mt.paint_id,\n#     mt.interior_id,\n#     mt.wheels_id,\n#     mt.audio_id,\n#     mt.roof_id,\n#     mt.tonneau_id,\n#     mt.accents_and_badging_id,\n#     mt.utility_panel_id,\n#     mt.location_name,\n#     mt.location_type,\n#     COALESCE(mt.inventory_policy_compliance, 'Not Compliant') as inventory_policy_compliance,\n#     COALESCE(mt.product_plan_compliance, 'Not Compliant') as product_plan_compliance,\n#     im.sku_classification\n#     FROM merged_table mt\n#     left join item_master im ON im.config_string = mt.mapped_my26_config_string\n#     LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(mt.order_history_zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     with cte as (\n#         SELECT order_id, \n#     oh.zip_code as original_order_history_zip_code, \n#     \n#     -- manual intervention\n#     case \n#     -- when oh.zip_code = '33076-4619' then '33076' \n#     WHEN POSITION('-' IN oh.zip_code) > 0 THEN SUBSTRING(oh.zip_code, 1, POSITION('-' IN oh.zip_code) - 1)\n#     WHEN SUBSTRING(oh.zip_code, 1, 1) = '0' THEN SUBSTRING(oh.zip_code, 2, LENGTH(oh.zip_code))\n#     else oh.zip_code end as order_history_zip_code,\n#     \n#     -- zc.customer_zip_code,\n#     -- zc.sap_sc_code,\n#     -- zc.sc_name,\n#     order_date,\n#     strftime('%Y-%m-%d', date_trunc('week', order_date)) as order_week,\n#     count(distinct order_week) over () as num_of_weeks,\n#     oh.config_string,\n#     mr.mapped_my26_config_string,\n#     \n#     -- COALESCE(mr.mapped_my26_config_string, oh.config_string) as mapped_my26_config_string, \n#     -- concat(COALESCE(mr.mapped_my26_config_string, oh.config_string), ' | ', zc.sc_name) as unique_config_string_location, \n#     -- realized if i don't filter to the distinct unique_config_string_location i'll just have a lot of repeat rows in my merged_df_2 output\n#     \n#     oh.vehicle_generation,\n#     COALESCE(mr.model_year, oh.model_year) as model_year,\n#     coalesce(mr.country, oh.country) as country, \n#     coalesce(mr.model_powertrain, oh.model_powertrain) as model_powertrain,\n#     coalesce(mr.paint_id, oh.paint_id) as paint_id,\n#     coalesce(mr.interior_id, oh.interior_id) as interior_id,\n#     coalesce(mr.wheels_id, oh.wheels_id) as wheels_id, \n#     coalesce(mr.audio_id, oh.audio_id) as audio_id,\n#     coalesce(mr.roof_id, oh.roof_id) as roof_id, \n#     coalesce(mr.tonneau_id, oh.tonneau_id) as tonneau_id, \n#     coalesce(mr.accents_and_badging_id, oh.accents_and_badging_id) as accents_and_badging_id,\n#     coalesce(mr.utility_panel_id, oh.utility_panel_id) as utility_panel_id,\n#     oh.location_name,\n#     COALESCE(im.inventory_policy_compliance, 'Not Compliant') as inventory_policy_compliance,\n#     COALESCE(pp.product_plan_compliance, 'Not Compliant') as product_plan_compliance,\n#     -- im.sku_classification,\n#     -- the problem with joining sku_classification metric from item master (im) to the newly mapped config string (mapped_my26_config_string) is it is possible that we have config_strings which have never had \n#     -- im guessing the original config string had a sku class, but what is its sku class as its newly mapped config string\n#     -- ADD CALCULATED SKU CLASS?\n#     CASE WHEN oh.location_name LIKE ('%Normal%') THEN 'Factory' ELSE 'Field' END AS location_type\n#     FROM order_history_df oh -- exists\n#     LEFT JOIN my_25_to_my_26_remapping mr ON oh.config_string = mr.config_string_original -- exists\n#     LEFT JOIN item_master im ON im.config_string = mr.mapped_my26_config_string\n#     LEFT JOIN product_plan pp ON pp.config_string = mr.mapped_my26_config_string\n#     -- LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(oh.zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     -- LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(oh.zip_code AS FLOAT) = CAST(zc.customer_zip_code AS FLOAT)\n#     \n#     -- the reason why we have mapped config_strings\n#     )\n#     select \n#     --cte.order_history_zip_code, -- removed to avoid duplicate column name\n#     zc.customer_zip_code,\n#     zc.sap_sc_code,\n#     zc.sc_name,\n#     concat(COALESCE(cte.mapped_my26_config_string, cte.config_string), ' | ', zc.sc_name) as unique_config_string_location, \n#     cte.*,\n#     t2.sku_classification as sku_classification\n#     from cte \n#     left join item_master t2 on cte.mapped_my26_config_string = t2.config_string\n#     LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(cte.order_history_zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     where 1=1\n#     and sap_sc_code != 8101\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## merged_df_2 the distinct unique_config_string_location with all the details from the merging in merged_df\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     -- Need to be join by location_type (Normal vs Field, model (R1S vs R1T), sku_classification \n#     \n#     with merged_table as (SELECT \n#     -- mr.mapped_my26_config_string, \n#     oh.config_string,\n#     -- concat(COALESCE(mr.mapped_my26_config_string, oh.config_string), ' | ', zc.sc_name) as unique_config_string_location,\n#     oh.config_string as original_config_string,\n#     coalesce(mr.mapped_my26_config_string, oh.config_string) as mapped_my26_config_string,\n#     \n#     \n#     -- oh.zip_code as order_history_zip_code,\n#     -- manual intervention to address how one single zip code causes an entire column to be read as a float\n#     case \n#     -- when oh.zip_code = '33076-4619' then '33076' \n#     WHEN POSITION('-' IN oh.zip_code) > 0 THEN SUBSTRING(oh.zip_code, 1, POSITION('-' IN oh.zip_code) - 1)\n#     WHEN SUBSTRING(oh.zip_code, 1, 1) = '0' THEN SUBSTRING(oh.zip_code, 2, LEN(oh.zip_code))\n#     else oh.zip_code end as order_history_zip_code,\n#     \n#     \n#     -- zc.customer_zip_code as zip_code_to_sc_mapping_code,\n#     -- zc.sap_sc_code,\n#     -- zc.sc_name,\n#     strftime('%Y-%m-%d', date_trunc('week', order_date)) as order_week,\n#     count(distinct order_week) over () as num_of_weeks,\n#     oh.config_string,\n#     mr.mapped_my26_config_string, \n#     -- concat(mr.mapped_my26_config_string, ' | ', oh.location_name) as unique_config_string_location, \n#     -- realized if i don't filter to the distinct unique_config_string_location i'll just have a lot of repeat rows in my merged_df_2 output\n#     oh.vehicle_generation,\n#     COALESCE(mr.model_year, oh.model_year) as model_year,\n#     coalesce(mr.country, oh.country) as country, \n#     coalesce(mr.model_powertrain, oh.model_powertrain) as model_powertrain,\n#     coalesce(mr.paint_id, oh.paint_id) as paint_id,\n#     coalesce(mr.interior_id, oh.interior_id) as interior_id,\n#     coalesce(mr.wheels_id, oh.wheels_id) as wheels_id, \n#     coalesce(mr.audio_id, oh.audio_id) as audio_id,\n#     coalesce(mr.roof_id, oh.roof_id) as roof_id, \n#     coalesce(mr.tonneau_id, oh.tonneau_id) as tonneau_id, \n#     coalesce(mr.accents_and_badging_id, oh.accents_and_badging_id) as accents_and_badging_id,\n#     coalesce(mr.utility_panel_id, oh.utility_panel_id) as utility_panel_id,\n#     oh.location_name,\n#     im.inventory_policy_compliance, \n#     pp.product_plan_compliance,\n#     -- im.sku_classification,\n#     -- ADD CALCULATED SKU CLASS?\n#     CASE WHEN oh.location_name LIKE ('%Normal%') THEN 'Normal' ELSE 'Field' END AS location_type\n#     FROM order_history_df oh\n#     LEFT JOIN my_25_to_my_26_remapping mr ON oh.config_string = mr.config_string_original\n#     LEFT JOIN item_master im ON im.config_string = mr.mapped_my26_config_string\n#     LEFT JOIN product_plan pp ON pp.config_string = mr.mapped_my26_config_string\n#     -- LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(oh.zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     )\n#     \n#     SELECT distinct concat(COALESCE(mt.mapped_my26_config_string, mt.original_config_string), ' | ', zc.sc_name) as unique_config_string_location,\n#     mt.mapped_my26_config_string,\n#     mt.original_config_string,\n#     mt.order_history_zip_code,\n#     zc.customer_zip_code as zip_code_to_sc_mapping_code,\n#     zc.sap_sc_code,\n#     zc.sc_name,\n#     mt.num_of_weeks,\n#     mt.vehicle_generation,\n#     mt.model_year,\n#     mt.country,\n#     substring(mt.model_powertrain, 1, 3) as model,\n#     mt.model_powertrain,\t\n#     mt.paint_id,\n#     mt.interior_id,\n#     mt.wheels_id,\n#     mt.audio_id,\n#     mt.roof_id,\n#     mt.tonneau_id,\n#     mt.accents_and_badging_id,\n#     mt.utility_panel_id,\n#     mt.location_name,\n#     mt.location_type,\n#     COALESCE(mt.inventory_policy_compliance, 'Not Compliant') as inventory_policy_compliance,\n#     COALESCE(mt.product_plan_compliance, 'Not Compliant') as product_plan_compliance,\n#     im.sku_classification\n#     FROM merged_table mt\n#     left join item_master im ON im.config_string = mt.mapped_my26_config_string\n#     LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(mt.order_history_zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     where 1=1\n#     and sap_sc_code != 8101\n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     with merged_table as (SELECT \n#     -- mr.mapped_my26_config_string, \n#     oh.config_string,\n#     -- concat(COALESCE(mr.mapped_my26_config_string, oh.config_string), ' | ', zc.sc_name) as unique_config_string_location,\n#     oh.config_string as original_config_string,\n#     coalesce(mr.mapped_my26_config_string, oh.config_string) as mapped_my26_config_string,\n#     \n#     \n#     -- oh.zip_code as order_history_zip_code,\n#     -- manual intervention to address how one single zip code causes an entire column to be read as a float\n#     case \n#     -- when oh.zip_code = '33076-4619' then '33076' \n#     WHEN POSITION('-' IN oh.zip_code) > 0 THEN SUBSTRING(oh.zip_code, 1, POSITION('-' IN oh.zip_code) - 1)\n#     WHEN SUBSTRING(oh.zip_code, 1, 1) = '0' THEN SUBSTRING(oh.zip_code, 2, LENGTH(oh.zip_code))\n#     else oh.zip_code end as order_history_zip_code,\n#     \n#     \n#     -- zc.customer_zip_code as zip_code_to_sc_mapping_code,\n#     -- zc.sap_sc_code,\n#     -- zc.sc_name,\n#     strftime('%Y-%m-%d', date_trunc('week', order_date)) as order_week,\n#     count(distinct order_week) over () as num_of_weeks,\n#     oh.config_string,\n#     mr.mapped_my26_config_string, \n#     -- concat(mr.mapped_my26_config_string, ' | ', oh.location_name) as unique_config_string_location, \n#     -- realized if i don't filter to the distinct unique_config_string_location i'll just have a lot of repeat rows in my merged_df_2 output\n#     oh.vehicle_generation,\n#     COALESCE(mr.model_year, oh.model_year) as model_year,\n#     coalesce(mr.country, oh.country) as country, \n#     coalesce(mr.model_powertrain, oh.model_powertrain) as model_powertrain,\n#     coalesce(mr.paint_id, oh.paint_id) as paint_id,\n#     coalesce(mr.interior_id, oh.interior_id) as interior_id,\n#     coalesce(mr.wheels_id, oh.wheels_id) as wheels_id, \n#     coalesce(mr.audio_id, oh.audio_id) as audio_id,\n#     coalesce(mr.roof_id, oh.roof_id) as roof_id, \n#     coalesce(mr.tonneau_id, oh.tonneau_id) as tonneau_id, \n#     coalesce(mr.accents_and_badging_id, oh.accents_and_badging_id) as accents_and_badging_id,\n#     coalesce(mr.utility_panel_id, oh.utility_panel_id) as utility_panel_id,\n#     oh.location_name,\n#     im.inventory_policy_compliance, \n#     pp.product_plan_compliance,\n#     -- im.sku_classification,\n#     -- ADD CALCULATED SKU CLASS?\n#     CASE WHEN oh.location_name LIKE ('%Normal%') THEN 'Normal' ELSE 'Field' END AS location_type\n#     FROM order_history_df oh\n#     LEFT JOIN my_25_to_my_26_remapping mr ON oh.config_string = mr.config_string_original\n#     LEFT JOIN item_master im ON im.config_string = mr.mapped_my26_config_string\n#     LEFT JOIN product_plan pp ON pp.config_string = mr.mapped_my26_config_string\n#     -- LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(oh.zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     ),\n#     \n#     \n#     \n#     cte_2 as (\n#     SELECT concat(COALESCE(mt.mapped_my26_config_string, mt.original_config_string), ' | ', zc.sc_name) as unique_config_string_location,\n#     mt.mapped_my26_config_string,\n#     mt.original_config_string,\n#     mt.order_history_zip_code,\n#     zc.customer_zip_code as zip_code_to_sc_mapping_code,\n#     zc.sap_sc_code,\n#     zc.sc_name,\n#     mt.num_of_weeks,\n#     mt.vehicle_generation,\n#     mt.model_year,\n#     mt.country,\n#     substring(mt.model_powertrain, 1, 3) as model,\n#     mt.model_powertrain,\t\n#     mt.paint_id,\n#     mt.interior_id,\n#     mt.wheels_id,\n#     mt.audio_id,\n#     mt.roof_id,\n#     mt.tonneau_id,\n#     mt.accents_and_badging_id,\n#     mt.utility_panel_id,\n#     mt.location_name,\n#     mt.location_type,\n#     COALESCE(mt.inventory_policy_compliance, 'Not Compliant') as inventory_policy_compliance,\n#     COALESCE(mt.product_plan_compliance, 'Not Compliant') as product_plan_compliance,\n#     im.sku_classification\n#     FROM merged_table mt\n#     left join item_master im ON im.config_string = mt.mapped_my26_config_string\n#     LEFT JOIN customer_zip_code_sc_mapping zc ON CAST(mt.order_history_zip_code AS VARCHAR) = CAST(zc.customer_zip_code AS VARCHAR)\n#     where 1=1\n#     and sap_sc_code != 8101\n#     )\n#     \n#     select distinct unique_config_string_location, \n#     mapped_my26_config_string,\n#     original_config_string,\n#     order_history_zip_code,\n#     zip_code_to_sc_mapping_code,\n#     sap_sc_code,\n#     sc_name,\n#     num_of_weeks,\n#     vehicle_generation,\n#     model_year,\n#     country,\n#     model,\n#     model_powertrain,\t\n#     paint_id,\n#     interior_id,\n#     wheels_id,\n#     audio_id,\n#     roof_id,\n#     tonneau_id,\n#     accents_and_badging_id,\n#     utility_panel_id,\n#     location_name,\n#     location_type,\n#     inventory_policy_compliance,\n#     product_plan_compliance,\n#     sku_classification\n#     from cte_2\n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"- (A) Calculate Average Weekly Demand -> what is the number of units demanded per config_string location between the user input dates?\n- (A) Calculate Annualized Demand -> will need to calculate Round(Average Weekly Demand * 52, 0)\n- (A) STD of Demand ->STD of demand between the user input dates\n- (B) Add Annual Demand Threshold -> User Input\n- (B) LT (Weeks) -> user input\n- (B) STD LT (Weeks) -> user input\n- (B) Service Level -> user input\n- (B) Max SS Coverage -> User Input\n- (C) Z Score -> NORMINV(Service Level)\n- (C) Raw Safety Stock Units (NORMSINV(Service Level))*SQRT((Lead Time (Weeks)*(STD of Demand^2))+((STD Lead Time (Weeks)*Average Weekly Demand)^2))\n- (C) Rounded Raw SS Units \n- (D) Rounded Raw SS Coverage (Rounded Raw SS Units/Average Weekly Demand)\n- (D) Final SS Units\n  - 1. If annualized demand <= Annual demand threshold --> Final SS Units = 0\n  - 2. If raw rounded raw SS coverage >= Max SS coverage --> Max SS coverage x average weekly demand --> round result to integer value\n  - 3. If raw rounded raw SS coverage < Max SS coverage --> use rounded raw SS Units\n- (D) Final SS Coverage Weeks (Hist) -> Final SS Units / Average Weekly Demand\n\n\n\n","metadata":{}},{"cell_type":"markdown","source":"Part (A) took me like a week\n\n- Lot of debugging and backtracking in realizing built in SQL functions are not applicable \n\n","metadata":{}},{"cell_type":"markdown","source":"## Part A (Calculating Average Weekly Demand and STD of Weekly Demand)\n\n","metadata":{}},{"cell_type":"markdown","source":"#### **Display the number of orders per config_string | location combination per day**\n\n","metadata":{}},{"cell_type":"markdown","source":"Follow Up (1) Follow up with Charles and Peter on MY25-MY26 mapping, still seeing some configs not show up \n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     -- this table shows\n#     -- on XYZ order_date, how many orders were there for this unique_config_string_location\n#     \n#     select \n#     order_date, \n#     order_week, \n#     num_of_weeks,\n#     unique_config_string_location, \n#     -- count(order_id) over (partition by unique_config_string_location, order_date) as num_orders_on_date\n#     count(order_id) as num_orders_on_date\n#         --    STDEV(count(order_id)) over (partition by unique_config_string_location) as std_of_demand\n#            -- this form of STDEV makes no sense\n#            -- for instance if you look at unique_config_string '2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL', you'll see that there has only been one order of it on 2025-01-16\n#            -- this means when you try to partition it, it will only see 1 row of orders, when in reality we want to see an entire X amount of rows for every day in the user selected range (ie. 2024-09-30 to 2025-06-02 should be 246 days -> therefore 245 rows of 0 orders)\n#            -- as a result we need to do things differently by\n#            -- (1) Produce a list of unique order_dates/order_weeks given the user's input\n#            -- (2) Cross join to a unique list of unique_config_string_locations that were ordered in the user's selected date range\n#            -- (3) This will give an output of: for every order_date/order_week, there is a row with each unique_config_string_location \n#            -- (4) so for instance if I filter to '2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL' I will now see 246 rows with only one row having num_orders_on_date as 1 -> this allows me to correctly calculate STDEV\n#     from merged_df\n#     -- where unique_config_string_location in ('2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL')\n#     group by all\n#     order by order_date asc, order_week asc, num_orders_on_date asc\n#     \n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     -- this table shows\n#     -- the Standard Deviation of each unique_config_string_location\n#     -- this needs to be a separate table because I need to perform a cross join so that every unique_config_string_location has a order value (even 0) for every order_date in the date range selected\n#     -- this was we can properly calculate STDEV\n#     \n#     \n#     WITH order_date_range_list AS (\n#       SELECT DISTINCT order_week\n#       FROM merged_df\n#       ORDER BY order_week ASC\n#     ), \n#     \n#     unique_config_location_list AS (\n#       SELECT DISTINCT unique_config_string_location\n#       FROM merged_df\n#     ),\n#     \n#     cross_join_table AS (\n#       SELECT *\n#       FROM order_date_range_list\n#       CROSS JOIN unique_config_location_list\n#     )\n#     \n#     SELECT \n#     ad1.order_date, \n#     ct.*, \n#     coalesce(ad1.num_orders_on_date,0) as num_orders_on_date,\n#     STDDEV(coalesce(ad1.num_orders_on_date,0)) over (partition by ct.unique_config_string_location) as std_of_daily_demand\n#     -- need to calculate STDEV by order_week not by order_date need to first establish order_week column then sum the daily_orders to be weekly\n#     FROM cross_join_table ct\n#     LEFT JOIN agg_daily_demand_view_df ad1 ON ct.unique_config_string_location = ad1.unique_config_string_location AND ct.order_week = ad1.order_week\n#     -- where ct.unique_config_string_location in ('2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL')\n#     ORDER BY ct.order_week ASC\n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### **Display the number of orders per config_string | location combination per week & average weekly orders & STDEV of orders per day**\n\n","metadata":{}},{"cell_type":"markdown","source":"Problems I realized:\n\n- We cannot just use a simple sum or average to understand average weekly orders\n  - We need to incorporate windows functions so we understand across the unique_config_string_location, what was the average num of orders per week\n- Additionally for STDEV, we need to somehow change the dataframe so that for each unique_config_string_location, there is a row for it on every single week, even if it is zero orders in order to properly calculate STDEV\n  - This will make the calculator slower though for sure unless I can think of a more efficient way to do this...\n  - The first solution I have in mind is to do a cross join between the range of dates (in the form of weeks) from the user's input and the unique_config_string_location list, and then coalesce the num_orders_on_date with 0 \n  - I also need to calculate the STDEV by weekly orders, not by daily orders\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     -- this table shows \n#     -- how many total orders are there per unique_config_string_location -> total_num_orders_per_config_string_location\n#     -- for every unique_config_string_location, ho wmany orders were placed per week \n#     -- your average number of orders per week (total_num_orders_per_config_string_location / avg_orders_per_week)\n#     -- need this table for joining to merged_df_2 for avg_weekly_demand\n#     -- we also need to calculate this by the distinct unique_config_string_location and order_week combination otherwise we will have repeating rows\n#     \n#     select \n#     distinct(concat(unique_config_string_location, ' | ', order_week)) as unique_config_string_location_order_week,\n#     order_week,\n#     unique_config_string_location,\n#     -- distinct unique_config_string_location,\n#     sum(num_orders_on_date) over (partition by unique_config_string_location) as total_num_orders_per_config_string_location,\n#     \n#     --NEW sum of num_orders_on_date over partition by unique_config_string_location, order_week -> the number of orders per week for each config_string_location \n#     sum(num_orders_on_date) over (partition by unique_config_string_location, order_week) as total_num_orders_per_config_string_location_per_week,\n#     \n#     num_of_weeks, \n#     sum(num_orders_on_date) over (partition by unique_config_string_location) / num_of_weeks as avg_orders_per_week\n#     -- -- sum of all orders for each config_string | location combination in the user selected timeframe divided by total number of weeks from the user selected timeframe\n#     -- STDEV(num_orders_on_date) over (partition by unique_config_string_location) as std_of_demand\n#     --    STDEV(count(order_id)) over (partition by unique_config_string_location) as std_of_demand\n#            -- this form of STDEV makes no sense\n#            -- for instance if you look at unique_config_string '2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL', you'll see that there has only been one order of it on 2025-01-16\n#            -- this means when you try to partition it, it will only see 1 row of orders, when in reality we want to see an entire X amount of rows for every day in the user selected range (ie. 2024-09-30 to 2025-06-02 should be 246 days -> therefore 245 rows of 0 orders)\n#            -- as a result we need to do things differently by\n#            -- (1) Produce a list of unique order_dates/order_weeks given the user's input\n#            -- (2) Cross join to a unique list of unique_config_string_locations that were ordered in the user's selected date range\n#            -- (3) This will give an output of: for every order_date/order_week, there is a row with each unique_config_string_location \n#            -- (4) so for instance if I filter to '2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL' I will now see 246 rows with only one row having num_orders_on_date as 1 -> this allows me to correctly calculate STDEV\n#     from agg_daily_demand_view_df\n#     -- where unique_config_string_location in ('2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL')\n#     order by unique_config_string_location asc, order_week asc\n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     -- this table shows\n#     -- we need to perform a cross join between the list of unique date_range in user selection and the list of unique_config_string_location \n#     -- so that we have a table where for every unique_config_string_location there is a order_week, even if it is zero\n#     -- we then merge the total_num_orders_per_config_string_location_per_week value calculated above and then calculate the stdev acros each unique_config_string_location \n#     \n#     \n#     WITH order_date_range_list AS (\n#       SELECT DISTINCT order_week\n#       FROM merged_df\n#       ORDER BY order_week ASC\n#     ), \n#     \n#     unique_config_location_list AS (\n#       SELECT DISTINCT unique_config_string_location\n#       FROM merged_df\n#     ),\n#     \n#     cross_join_table AS (\n#       SELECT unique_config_string_location, order_week\n#     --   concat(unique_config_string_location, ' | ', order_week) as unique_config_string_location_order_week\n#       FROM order_date_range_list\n#       CROSS JOIN unique_config_location_list\n#     -- at this point we now have for every order_week in the user selected range, a row that represents each unique_config_string_location that has ever come through in the range selected \n#     )\n#     \n#     \n#     -- select * from cross_join_table\n#     -- order by unique_config_string_location asc, order_week asc\n#     \n#     \n#     SELECT \n#     ct.*,\n#     -- distinct ct.unique_config_string_location,\n#     -- coalesce(ad1.total_num_orders_per_config_string_location,0) as total_num_orders_on_week_per_config_string_location,\n#     STDDEV(coalesce(ad1.total_num_orders_per_config_string_location_per_week,0)) over (partition by ct.unique_config_string_location) as std_of_weekly_demand,\n#     -- need to calculate STDEV by order_week not by order_date need to first establish order_week column then sum the daily_orders to be weekly\n#     coalesce(ad1.total_num_orders_per_config_string_location,0) as total_num_orders_per_config_string_location, \n#     coalesce(ad1.total_num_orders_per_config_string_location_per_week,0) as total_num_orders_per_config_string_location_per_week\n#     -- ad1.num_of_weeks,\n#     -- ad1.avg_orders_per_week\n#     FROM cross_join_table ct\n#     LEFT JOIN agg_weekly_demand_view_df ad1 ON ct.unique_config_string_location = ad1.unique_config_string_location AND ct.order_week = ad1.order_week\n#     -- where ct.unique_config_string_location in ('2026_US_R1T Dual Large_EXP-MDN_INT-GYP_WHL-2SD_AUD-P01_ROOF-DGP_TON-P02_ACTBDG-DRK_UTL-T01 | SC-Chicago, IL')\n#     -- where ct.unique_config_string_location in ('2026_US_R1S Dual Standard_EXP-GWT_INT-BMP_WHL-0BS_AUD-STD_ROOF-FGP_TON-NULL_ACTBDG-BRT_UTL-S01 | SC - EASTVALE, CA')\n#     ORDER BY ct.unique_config_string_location asc, ct.order_week ASC\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Part B (Joining Data Values to merged_df table)**\n\n- (A) Calculate Average Weekly Demand -> what is the number of units demanded per config_string location between the user input dates? -> Table: agg_weekly_demand_view_df, column: avg_orders_per_week\n- (A) Calculate Annualized Demand -> will need to calculate Round(Average Weekly Demand * 52, 0) -> Table: agg_weekly_demand_view_df, column: avg_orders_per_week * 52\n- (A) STD of Demand ->STD of demand between the user input dates -> Table: weekly_demand_std_df, column: std_of_weekly_demand\n\n\n\n","metadata":{}},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     with distinct_agg_weekly_demand_table as (\n#     select distinct unique_config_string_location, avg_orders_per_week\n#     from agg_weekly_demand_view_df\n#     ),\n#     \n#     std_weekly_demand_table as (\n#     select distinct unique_config_string_location, std_of_weekly_demand\n#     from weekly_demand_std_df\n#     )\n#     \n#     -- select *\n#     -- -- from distinct_agg_weekly_demand_table\n#     -- from std_weekly_demand_table\n#     \n#     select md.*, \n#     ROUND(wd.avg_orders_per_week,4) as average_weekly_demand, \n#     ROUND((wd.avg_orders_per_week*52),4) as annualized_demand,\n#     ROUND(sd.std_of_weekly_demand,4) as standard_deviation_of_weekly_demand\n#     from merged_df_2 md\n#     left join distinct_agg_weekly_demand_table wd on md.unique_config_string_location = wd.unique_config_string_location\n#     left join std_weekly_demand_table sd on md.unique_config_string_location = sd.unique_config_string_location\n#     \n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## **Join the Below User Inputs:**\n\n- (B) Add Annual Demand Threshold -> User Input\n  - service_level_annual_demand_ss_coverage -> annual_demand_threshold\n- (B) LT (Weeks) -> user input\n  - replenishment_lead_time_to_normal\n- (B) STD LT (Weeks) -> user input\n  - lead_time_std_for_field\n- (B) Service Level -> user input\n  - service_level_annual_demand_ss_coverage -> service_level\n- (B) Max SS Coverage -> User Input\n  - service_level_annual_demand_ss_coverage -> ss_coverage_max\n\n\n\nNeed to be join by location_type (Normal vs Field, model (R1S vs R1T), sku_classification \n\n\n\n","metadata":{}},{"cell_type":"code","source":"merged_df_3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"service_level_annual_demand_ss_coverage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"field_lead_times","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import jinja2\n# raw_query = \"\"\"\n#     select md.*, substring(md.sku_classification, 1, 1) as sku_classification_letter, ss.service_level, ss.annual_demand_threshold, ss.ss_coverage_max, ft.total_lead_time\n#     from merged_df_3 md\n#     left join service_level_annual_demand_ss_coverage ss on (ss.location_type = md.location_type AND ss.model = md.model AND ss.sku_classification = substring(md.sku_classification, 1, 1))\n#     left join field_lead_times ft on ft.location_id = md.sap_sc_code\n# \"\"\"\n# sql_query = jinja2.Template(raw_query).render(vars())","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def assign_replenishment_lead_time(df:pd.NA):\n    df['replenishment_lead_time_to_normal'] = pd.NA\n    df['lead_time_std'] = pd.NA\n\n    df.loc[df['sap_sc_code'].isin(['8101']), 'replenishment_lead_time_to_normal'] = replenishment_lead_time_to_normal\n    df.loc[df['sap_sc_code'].isin(['8101']), 'lead_time_std'] = lead_time_std_for_normal * df['replenishment_lead_time_to_normal']\n\n    df.loc[~df['sap_sc_code'].isin(['8101']), 'replenishment_lead_time_to_normal'] = df['total_lead_time']\n    df.loc[~df['sap_sc_code'].isin(['8101']), 'lead_time_std'] = lead_time_std_for_field * df['total_lead_time']\n#this needs to be lead_time_std_for_field * total_lead_time\n#if an order was delivered out of Normal ignore it?\n\n\n    return df\n\nmerged_df_5 = assign_replenishment_lead_time(merged_df_4)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df_5","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"service_level_annual_demand_ss_coverage","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Calculate Part C and Part D\n\n- (C) Z Score -> NORMINV(service_level)\n  - service_level is in percentage form ie 90%\n  - do we need to convert to 0.9? yes\n- (C) Raw Safety Stock Units (NORMSINV(Service Level))*SQRT((Lead Time (Weeks)*(STD of Demand^2))+((STD Lead Time (Weeks)*Average Weekly Demand)^2))\n- (C) Rounded Raw SS Units \n- (D) Rounded Raw SS Coverage (Rounded Raw SS Units/Average Weekly Demand)\n- (D) Final SS Units\n  - 1. If annualized demand <= Annual demand threshold --> Final SS Units = 0\n  - 2. If raw rounded raw SS coverage >= Max SS coverage --> Max SS coverage x average weekly demand --> round result to integer value\n  - 3. If raw rounded raw SS coverage < Max SS coverage --> use rounded raw SS Units\n- (D) Final SS Coverage Weeks (Hist) -> Final SS Units / Average Weekly Demand\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom scipy.stats import norm\n\ndef calculate_safety_stock(df: pd.DataFrame):\n    df['z_score'] = norm.ppf(df['service_level']).round(3)\n\n    # df['raw_safety_stock_units'] = (norm.ppf(df['service_level']).round(3) * (df['replenishment_lead_time_to_normal'] * (df['standard_deviation_of_weekly_demand'] ** 2)) ** 0.5 + (df['lead_time_std'] * df['average_weekly_demand']) ** 2)\n\n    #corrected raw safety stock units\n    df['raw_safety_stock_units'] = (norm.ppf(df['service_level']) * ((df['replenishment_lead_time_to_normal'] * (df['standard_deviation_of_weekly_demand'] ** 2)) + ((df['lead_time_std'] * df['average_weekly_demand']) ** 2)) ** 0.5).round(3) \n\n\n    # df['rounded_safety_stock_units'] = (norm.ppf(df['service_level']).round(3) * (df['replenishment_lead_time_to_normal'] * (df['standard_deviation_of_weekly_demand'] ** 2)) ** 0.5 + (df['lead_time_std'] * df['average_weekly_demand']) ** 2).round(0)\n#make sure this is rounding correctly\n    #corrected rounded safety stock units\n    df['rounded_safety_stock_units'] = (norm.ppf(df['service_level']) * ((df['replenishment_lead_time_to_normal'] * (df['standard_deviation_of_weekly_demand'] ** 2)) + ((df['lead_time_std'] * df['average_weekly_demand']) ** 2)) ** 0.5).round(0)\n\n    df['rounded_safety_stock_units'] = df['rounded_safety_stock_units'].round(0).astype(int)\n\n    df['rounded_safety_stock_coverage'] = (df['rounded_safety_stock_units'] / df['average_weekly_demand']).round(0).astype(int)\n\n    return df\n\nmerged_df_6 = calculate_safety_stock(merged_df_5)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"2026_US_R1S Tri Max_EXP-CDN_INT-SND_WHL-0DD_AUD-P01_ROOF-DGP_TON-NULL_ACTBDG-DRK_UTL-S01\n-> unique_config_string_location has repeating rows\n\n","metadata":{}},{"cell_type":"code","source":"merged_df_6","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- **Safety Stock Calculator**\n- (D) Final SS Units\n  - 1. If annualized demand <= Annual demand threshold --> Final SS Units = 0\n  - 2. If raw rounded raw SS coverage >= Max SS coverage --> Max SS coverage x average weekly demand --> round result to integer value\n  - 3. If raw rounded raw SS coverage < Max SS coverage --> use rounded raw SS Units\n- (D) Final SS Coverage Weeks (Hist) -> Final SS Units / Average Weekly Demand\n\n\n\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np # Import numpy for np.inf and pd.NA\n\ndef custom_row_function(row):\n    # Condition 1: annualized_demand <= annual_demand_threshold\n    if row['annualized_demand'] <= row['annual_demand_threshold']:\n        return 0 \n    # Condition 2: rounded_safety_stock_coverage >= ss_coverage_max\n    elif row['rounded_safety_stock_coverage'] >= row['ss_coverage_max']:\n        return (row['ss_coverage_max'] * row['average_weekly_demand']).round(0)\n    # Condition 3: rounded_safety_stock_coverage < ss_coverage_max\n    elif row['rounded_safety_stock_coverage'] < row['ss_coverage_max']:\n        return row['rounded_safety_stock_units']\n    else:\n        return pd.NA\n\ndef calculate_final_safety_stock(df:pd.DataFrame):\n    df_copy = df.copy() # Always work on a copy\n\n    # Apply the custom function to create 'final_safety_stock_units' column\n    # axis=1 means the function is applied row by row\n    df_copy['final_safety_stock_units'] = df_copy.apply(custom_row_function, axis=1)\n\n    # Calculate final_safety_stock_coverage_weeks as a separate vectorized operation\n    # Handle division by zero\n    df_copy['final_safety_stock_coverage_weeks'] = (\n        df_copy['final_safety_stock_units'] / df_copy['average_weekly_demand']\n    ).replace([np.inf, -np.inf], np.nan).fillna(0) # Replace inf with NaN, then fill remaining NaNs with 0\n\n    return df_copy\n\n\nmerged_df_7 = calculate_final_safety_stock(merged_df_6)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Safety Stock Calculations**\n\n","metadata":{}},{"cell_type":"code","source":"merged_df_8 = merged_df_7.drop_duplicates(subset=['unique_config_string_location'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"merged_df_8","metadata":{},"execution_count":null,"outputs":[]}],"metadata":{"orig_nbformat":4,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"hex_info":{"author":"Ethan Wang","project_id":"0197cc42-6089-7003-bf1a-0d83acfe7b3f","version":"draft","exported_date":"Sun Aug 24 2025 20:58:48 GMT+0000 (Coordinated Universal Time)"}},"nbformat":4,"nbformat_minor":4}